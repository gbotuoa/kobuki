<!DOCTYPE html>
<html>
<head>
<style>

	html {
		margin 0; padding 0;
		background-color: black;
		color: #eee;
		font-family: sans-serif;
	}

	body {
		margin: 1em; padding: 1em;
		background-color: #333;
		font-size: 2em;
	}

	h1, h2, h3, h4, h5 {
		text-align: center;
	}

</style>
</head>
<body>

	<h1>Robot 9000</h1>

	<h2>Wandering Around</h2>

	<p>Initially, I divided the laser range scan into four equal arcs. I looped
	through the ranges and found the nearest point detected in each span and if
	that point was within a certain threshold ( range < 1.0 ) then I would mark
	that ark as being obstructed using a bitmask. Unfortunately, using four
	arcs caused complications. For example, givin the obstructions 1101 and 1011
	the robot would "get stuck" between swiviling left and right. Reducing the
	number of arcs to three, (left, center, and right) solved most of the
	problems.

	<p>The final dictionary of motions I sent to the robot were as follows:

	<p><pre>
	self.move = {
	   0: (math.sin(time.time()), fast),
	   1: (-medi, medi),
	  10: (fast*rdir(), stop),
	  11: (-fast, slow),
	 100: ( medi, medi),
	 101: ( stop, slow),
	 110: (-fast, stop),
	 111: (-fast, stop),
	"stop": (stop, stop)
	}</pre></p>

	<p>The first element in the tuple specifies the angular direction and the
	second specifies the forward/backward movement.

	<p>When their was no obstruction, the robot would follow a sinusoidal
	path to add an element of randomness. The rdir() function, which was invoked
	when an obstacle was detected dead ahead, provided a random direction
	continuously until a delay threshold was met, where upon it would select
	a new direction.


	<h2>Following a point</h2>

	<p>The solution that led our team to our massive victory was somewhat naive,
	but thoroughly effective. From the laser scan data I initially found the
	nearest point. Thereafter, I would using a growing window from the previous
	point until a new point was found nearby. The magic was that independent of
	range, the x position needed to be nearby as well.

	<p><pre>
    d1 = 100000
    a1 = 100000
    dx = 100000
    dy = 100000

    point_valid = 0

    for w in numpy.linspace(0.05,0.2,15):
        for i in range(len(scan_data.ranges)):
            if d1 > scan_data.ranges[i]:
                if prev_point is None:
                    d1 = scan_data.ranges[i]
                    a1 = a_list[i]
                    point_valid = 1
                else:
                    p = scan_data.ranges[i]
                    if abs((p * numpy.cos(a_list[i])) - prev_point.x) < w:
                        d1 = scan_data.ranges[i]
                        a1 = a_list[i]
                        point_valid = 1

        if point_valid:
            print "W", w
            break;

    if point_valid:

        point.x = d1 * numpy.cos(a1)
        point.y = d1 * numpy.sin(a1)
        pub.publish(point)
	</pre></p>

	<p>This resulted in very accurate tracking, as both the distance required to
	be minimized for a point to be followed, and as well the actual and absolute
	horizontal position needed to be self similar to the previous point. To
	reiterate, this implicitly tracked the distance as well, as the following
	diagram illustrates:</p>


	<p>This resulted in very accurate tracking, as both the distance and
	direction were tracked as the actual horizontal position needed to be self
	similar to the previous point. To reiterate, this implicitly tracked the
	position and distance, as the following diagram illustrates:</p>

	
	<img src="follower-martin.svg">

	<p>From this point data, a simple PD controller node was used to attempt
	to keep the target point at 0.75m distance and central to the camera POV.



	</svg>

	<h2>Some trials</h2>
	<p>Idea: The way the original follower code doesn't work was beacuse the
	tracking point is easily distracted by whatever falls in that detection
	region. On the lab floor where all the chair legs and random goods around,
	the following wasn't very satisfying. Therefore I decided to work on a
	different approaches.</p>

  <p>Implementation: My approach was to do with vision. I decided to use
	camshif tracker at the first sight, because it caputres the histogram of the
	first scene robot "sees", and track its movement in the later
	frames. Detection only happens in the first sight, and later it tracks the
	movement of the firt detected contents. The tracking area was
	set according to the estimated height of an evading robot and the area it
	would appear on the screen. Also, in order not to collide or bump into
	other things, I added up a simple range detection using laserscan data. If
	anything falls within near 0.4 meters, robot will back up a little bit and
	turn the other way where object is. In any case, if robot loses track, it will
	automatically start searching around until it finds object again. This part
	was adapted from our wander bot.</p>

  <p>Results: For tracking large black and condensed black object, this
	algorithm works okay. But for a fast-moving object, this didn't give
	as well performances.</p>

  <p>Lesson learned: Maybe tracking algorithm works better for video analysis,
	but not real time implementation. In real time, each frame should go through
	an independent detection. </p>

	<h2>The other solution</h2>
	<p>
		The idea I had to track a robot is to use image threshold to extract things in black, and then use morphological transformation to remove all the noises. Once we got all in black in the image, then use the image moments to get the centroid point of the largest black block. Store the coordinate of the centroid point x and y into a data structure that has x, y and z. Then, use the depth image to get the distance between two robotics. Fill the z into the last parameter of the data structure and then publish the topic of the point coordinate. To detect if the follower need to turn, compute the subtraction of the x to see if the angle is big enough to turn. To detect if the follower need to move forward or backward, compute the subtraction of the z. 
	</p>
	<p>
		 The challenge to this method is that it always select the biggest black block in the image and sometimes the target varies. So, it may result in unstable target to follower. Then I tried to combine depth and black blocks together to determine which target to follow. Once we got the things in black in the image, store them into a list of data structure. Then get all depth to the centroid points that exist in the list. After that, compare the value of z and get the smallest one and then decide the centroid point with the smallest distance is the target point. However, due to the time limit, choosing the point that has smallest depth didn't finish.
	</p>
	<p>
		There is a source code in github that has similar idea I have for tracking the evader. However, the good part that I didn't think of is the measurement to get the value of depth of the centroid point. When we get the coordinate of the centroid point, instead of just get the depth value according to the value of x and y of the centroid point, it calculates the average of depth of the points around the centroid point as the depth of the centroid point. This can avoid the influence of noisy depth image and get an accurate value.
	</p>
	<p>
		The URL of the source code is: https://github.com/jjones646/turtlebot-follower
	</p>


</body>
</html>
